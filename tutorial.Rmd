---
title: "deepG tutorial"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
  html_document:
    theme: united
    toc: yes
---

```{r setup, include=FALSE}
set.seed(123)
library(hdf5r)
library(magrittr)
library(ggplot2)
library(deepG)
```

# Introduction

The deepG library can be used for applying deep learning on genomic data. The library supports creating neural network architecture,
automation of data preprocesing (data generator), network training, inference and visualizing feature importances (integrated gradients). 

# Create a model

deepG supports three functions to create a keras model. 

## `create_model_lstm_cnn`

The architecture of this model is $k$ * LSTM, $m$ * CNN and $n$ * dense layers, where $k,m \ge 0$ and $n \ge 1$.   
The user can choose the size of the individual LSTM, CNN and Dense layers and add additional features to each layer;
for example the LSTM layer may be bidirectional (runs input in two ways) or stateful (considers dependencies between batches).  

The last dense layer layer has a softmax activation and determines how many targets we want to predict. This output gives a vector of probabilities, i.e. the sum 
of the vector is 1 and each entry is a probabilty for one class.  

The following implementation creates a model with 3 CNN layer (+ batch normalization), 1 LSTM and 1 dense layer.  
```{r}
model <- create_model_lstm_cnn(
  maxlen = 500, # number of nucleotides processed in one sample
  layer_lstm = c(32), # number of LSTM cells
  layer_dense = c(4), # number of neurons in last layer (4 targets: A,C,G,T)
  vocabulary.size = 4, # input vocabulary has size 4 (A,C,G,T)
  kernel_size = c(12, 12, 12), # size of individual CNN windows for each layer
  filters = c(32, 64, 64), # number of CNN filters per layer
  pool_size = c(3, 3, 3) # size of max pooling per layer
)
```

The model expects an input with dimensions (NULL (batch size), maxlen, vocabulary size) and a target with dimension (NULL (batch size), number of targets).
Maxlen specifies the length of the input sequence. 

```{r warning=FALSE}
batch_size <- 3
maxlen <- 500
vocabulary.size <- 4
input <- array(rnorm(maxlen * batch_size * vocabulary.size), 
               dim = c(batch_size, maxlen, vocabulary.size))
pred <- predict(model, input) # make a prediction with random data
dim(pred) 
colnames(pred) <- c("A", "C", "G", "T") 
pred # prediction for initial random weights
``` 

## `create_model_lstm_cnn_target_middle`

This architecture is closely related to `create_model_lstm_cnn_target` with the main difference that the model has two input layers (provided `label_input = NULL`).  

```{r}
model <- create_model_lstm_cnn_target_middle(
  maxlen = 500,
  layer_lstm = c(32),
  layer_dense = c(4),
  vocabulary.size = 4,
  kernel_size = c(12, 12, 12),
  filters = c(32, 64, 64),
  pool_size = c(3, 3, 3)
)
```

This architecture can be used to predict a character in the middle of a sequence. For example 

sequence: \texttt{ACCG}\texttt{\color{blue}T}\texttt{GGAA}

then the first input should correspond to \texttt{ACCG}, the second input to \texttt{GGAA} and \texttt{T} to the target.  
This can be used to combine the 2 tasks 

1. predict \texttt{T} given \texttt{ACCG} 

2. predict \texttt{T} given \texttt{AAGG} (note reversed order of input)

in one model.

## `create_model_wavenet`

This model uses causal dilated convolution layers, which is suitable to handle long sequences. The original paper can be found [here](https://arxiv.org/pdf/1609.03499.pdf)  

```{r warning = FALSE}
model <- create_model_wavenet(filters = 16, kernel_size = 2, residual_blocks = 2^(2:4),
                              maxlen = 500, input_tensor = NULL, initial_kernel_size = 32,
                              initial_filters = 32, output_channels = 4,
                              output_activation = "softmax", solver = "adam",
                              learning.rate = 0.001, compile = TRUE) 
model
```

The model expects an input and output of dimension (batch size, maxlen, vocabulary.size). The target sequence should be equal to input sequence shifted by 
one position. For example, given a sequence \texttt{ACCGGTC} and maxlen = 6, the input should correspond to \texttt{ACCGGT} and target to \texttt{CCGGTC}. 

# Training 

## Preparing the data

Input data must be files in FASTA or FASTQ format and file names must have .fasta or .fastq ending; otherwise files will be ignored. 
All training and validation data should each be in one folder. deepG uses a data generator to iterate over files in train/validation folder.   
Before we train our model, we have to decide what our training objetive is. It can be either a language model or label classification.

```{r warning = FALSE}
path <- "/home/rmreches/tutorial"
path_16S_train <- file.path(path, "16s/train")
path_16S_validation <- file.path(path, "16s/validation")
path_bacteria_train <- file.path(path, "bacteria/train")
path_bacteria_validation <- file.path(path, "bacteria/validation")

checkpoint_path <- file.path(path, "checkpoints")
tensorboard.log <- file.path(path, "tensorboard")
dir_path <- file.path(path, "outputs")
if (!dir.exists(checkpoint_path)) dir.create(checkpoint_path)
if (!dir.exists(tensorboard.log)) dir.create(tensorboard.log)
if (!dir.exists(dir_path)) dir.create(dir_path)
```

## Language model

With language model, we mean a model that predicts a character in a sequence.
The target can be at the end of the sequence, for example

\texttt{ACGTCA}\texttt{\color{blue}G}

or in the middle

\texttt{ACG}\texttt{\color{blue}T}\texttt{CAG}

### Language model for 16S (predict next character) 

Say we want to predict the next character in a sequence given the last 500 characters and our text consists of the letters \texttt{A,C,G,T}. First we have to create a model. We may use a model with 1 LSTM, 3 CNN and 1 dense layer for predictions.   

```{r warning = FALSE}
model <- create_model_lstm_cnn(
  maxlen = 500,
  layer_lstm = c(32),
  layer_dense = c(4),
  vocabulary.size = 4,
  kernel_size = c(12, 12, 12),
  filters = c(32, 64, 64),
  pool_size = c(3, 3, 3),
  learning.rate = 0.001
)
```

Next we have to specify the location of our training and validation data and the output format of the data generator    

```{r warning = FALSE, message = FALSE}

trainNetwork(train_type = "lm", # train a language model
             model = model,
             path = path_16S_train, # location of trainig data
             path.val = path_16S_validation, # location of validation data
             checkpoint_path = checkpoint_path,
             tensorboard.log = tensorboard.log,
             validation.split = 0.2, # use 20% of samples for validation compared to train size
             run.name = "lm_16S_target_right",
             batch.size = 256, 
             epochs = 4, 
             steps.per.epoch = 10, # 1 epoch = 10 batches
             step = 500, # take a sample every 500 steps
             output = list(none = FALSE,
                           checkpoints = TRUE,
                           tensorboard = TRUE,
                           log = FALSE,
                           serialize_model = FALSE,
                           full_model = FALSE
             ),
             tb_images = TRUE,
             output_format = "target_right" # predict target at end of sequence 
             )

tensorflow::tensorboard(tensorboard.log)
```

### Predict character in middle of sequence 

If we want to predict a character in the middle of a sequence and use LSTM layers, we should split our input into two layers. One layer handles the sequence 
before and one the input after the target. If, for example 

sequence: \texttt{ACCG}\texttt{\color{blue}{T}}\texttt{GGAA} 

then first input corresponds to \texttt{ACCG} and second to \texttt{AAGG}. We may create a model with two input layers using the `create_model_cnn_lstm_target_middle`

```{r warning = FALSE}
model <- create_model_lstm_cnn_target_middle(
  maxlen = 500,
  layer_lstm = c(32),
  layer_dense = c(4),
  vocabulary.size = 4,
  kernel_size = c(12, 12, 12),
  filters = c(32, 64, 64),
  pool_size = c(3, 3, 3),
  learning.rate = 0.001
)
```

The `trainNetwork` call is identical to the previous model, except we have to change the output format of the generator by setting `output_format = "target_middle_lstm"`. This reverses the order of the sequence after the target.  

```{r warning = FALSE, message = FALSE}

trainNetwork(train_type = "lm", # train a language model
             model = model,
             path = path_16S_train, # location of trainig data
             path.val = path_16S_validation, # location of validation data
             checkpoint_path = checkpoint_path,
             tensorboard.log = tensorboard.log,
             validation.split = 0.2, # use 20% of samples for validation compared to train size
             run.name = "lm_16S_target_middle_lstm",
             batch.size = 256, 
             epochs = 4, 
             steps.per.epoch = 10, # 1 epoch = 10 batches
             step = 500, # take a sample every 500 steps
             output = list(none = FALSE,
                           checkpoints = TRUE,
                           tensorboard = TRUE,
                           log = FALSE,
                           serialize_model = FALSE,
                           full_model = FALSE
             ),
             tb_images = TRUE,
             output_format = "target_middle_lstm" # predict character in middle of sequence  
             )

```

## Label classification

With label classification, we describe the task of mapping a label to a sequence. 
For example: given the sequence \texttt{ACGACCG}, does the sequence belong to a viral or bacterial genome?

deepG offers two options to map a label to a sequence 

1. the label gets read from the fasta header

2. files from every class are in seperate folders

### Label by folder

We put all data from one class into separate folders. In the following example, we want to classify if a sequence belongs to 16s or bacterial genome. We have to put all 16s/bacteria files into their own folder. In this case the `path` and `path.val` arguments should be vectors, where each entry is the path to one class.    
```{r warning = FALSE}
model <- create_model_lstm_cnn(
  maxlen = 500,
  layer_lstm = c(32),
  layer_dense = c(2), # predict two classes 
  vocabulary.size = 4,
  kernel_size = c(12, 12, 12),
  filters = c(32, 64, 64),
  pool_size = c(3, 3, 3),
  learning.rate = 0.001
)
```

```{r warning = FALSE, message = FALSE}
trainNetwork(train_type = "label_folder", # reading label from folder  
             model = model,
             path = c(path_16S_train, # note that path has two entries 
                      path_bacteria_train), 
             path.val = c(path_16S_validation,
                          path_bacteria_validation),
             checkpoint_path = checkpoint_path,
             tensorboard.log = tensorboard.log,
             validation.split = 0.2, 
             run.name = "16S_vs_bacteria",
             batch.size = 256, # half of batch is 16s and other half bacteria data
             epochs = 6, 
             steps.per.epoch = 25, 
             step = 500,
             labelVocabulary = c("16s", "bacteria"), # label names
             output = list(none = FALSE,
                           checkpoints = TRUE,
                           tensorboard = TRUE,
                           log = FALSE,
                           serialize_model = FALSE,
                           full_model = FALSE
             ),
             tb_images = TRUE,
             proportion_per_file = c(1, 0.05) # randomly select 5% of bacteria file 
)
```

# Inference

Once we have trained a model, we may use the model to get the activations of a certain layer and write the states to an h5 file.
In the following example we use the binary model trained to classify 16S/bacteria data.

```{r warning = FALSE, message = FALSE}
print(model)
num_layers <- length(model$get_config()$layers)
layer_name <- model$get_config()$layers[[num_layers]]$name
cat("get output at layer", layer_name)
fasta.path <- list.files(path_16S_validation, full.names = TRUE)[1] # make predictions for 16S file
fasta.file <- microseq::readFasta(fasta.path)
head(fasta.file)
sequence <- fasta.file$Sequence[1]
filename <- file.path(dir_path, "states.h5")

if (!file.exists(filename)) {
  writeStates(
    model = model,
    layer_name = layer_name, 
    sequence = sequence,
    round_digits = 4,
    filename = filename,
    batch.size = 10,
    mode = "lm")
}  
```

We can access the h5 file as follows

```{r warning = FALSE, message = FALSE}
states <- readRowsFromH5(h5_path = filename, complete = TRUE)
colnames(states) <- c("16S", "bacteria")
head(states)
```

The matrix shows the models confidence in its predictions. Every row corresponds to one sample. If the value in the 
16s column is > 0.500, the model will classify the sample as 16s.

### Inference II 

We can use or trained model to detect 16S sequences in a bacterial genome. First, we search for the true rRNA region in the corresponding gff file.

```{r warning = FALSE, message = FALSE}
fasta.path<- file.path(path, "E_faecalis.fasta")
gff.file <- file.path(path, "E_faecalis.gff")
gff.data <- rtracklayer::readGFF(gff.file, version = 0,
                                 columns = NULL, tags = NULL, filter = NULL, nrows = -1,
                                 raw_data = FALSE)
rRNA_index <- stringr::str_detect(gff.data$product, "^16S ribosomal") & (gff.data$strand == "+")
start <- gff.data[rRNA_index, "start"]
end <- gff.data[rRNA_index, "end"]
start; end
```

We iterate over the bacteria file and make a predictions every 100 steps

```{r warning = FALSE, message = FALSE}
fasta.file <- microseq::readFasta(fasta.path)
sequence <- fasta.file$Sequence[1]
filename <- file.path(dir_path, "bacteria_states.h5")

if (!file.exists(filename)) {
  writeStates(
    model = model,
    layer_name = layer_name, 
    sequence = sequence,
    round_digits = 4,
    filename = filename,
    batch.size = 500,
    step = 100)
}  
```

```{r warning = FALSE, message = FALSE}
states <- readRowsFromH5(h5_path = filename, complete = TRUE, getTargetPositions = TRUE)
pred <- states[[1]]
position <- states[[2]] - 1
df <- cbind(pred, position) %>% as.data.frame()
colnames(df) <- c("conf_16S", "conf_bacteria", "seq_end")
head(df)
index_16S_pred <- df[ , 1] > 0.5
df_16S <- df[index_16S_pred, ]
df_16S
```

Let's visualize or models predictions and compare them to the true areas. First we look at the confidence in 16S over the whole genome. 

```{r warning = FALSE, message = FALSE}
ggplot(df, aes(x = seq_end, y = conf_16S)) + geom_point() + geom_line() + ylab("16S confidence")
```

Next we may zoom into areas with high 16S confidence.

```{r warning = FALSE, message = FALSE}
p1 <- ggplot(df, aes(x = seq_end, y = conf_16S)) + geom_point() + geom_line() +
  geom_rect(aes(xmin=start[1], xmax=end[1], ymin=0, ymax=Inf),  alpha = 0.01) + 
  xlim(c(start[1] - 1000, end[1] + 1000)) + 
  ylab("16S confidence") + xlab("sequence position")
p2 <- ggplot(df, aes(x = seq_end, y = conf_16S)) + geom_point() + geom_line() +
  geom_rect(aes(xmin=start[2], xmax=end[2], ymin=0, ymax=Inf),  alpha = 0.01) + 
  xlim(c(start[2] - 1000, end[2] + 1000)) + 
  ylab("16S confidence") + xlab("sequence position")
ggpubr::ggarrange(p1, p2, ncol = 1, nrow = 2)
```

## Tensorboard

We can use tensorboard to monitor our training runs. To track the runs, we have to specify a path for tensorboard files and give the run a unique name.   

```{r warning = FALSE, message = FALSE}
# tensorboard_path <- tensorboard.log
# if (!dir.exists(tensorboard_path)) dir.create(tensorboard_path)
# model <- create_model_lstm_cnn()
# run.name <- "run_1"
# trainNetwork(train_type = "lm", 
#              model = model,
#              path = train_path_1,
#              path.val = validation_path_1,
#              steps.per.epoch = 5, 
#              batch.size = 8,
#              epochs = 10,
#              run.name = run.name,
#              tensorboard.log = tensorboard_path,
#              output = list(none = FALSE,
#                            checkpoints = FALSE,
#                            tensorboard = TRUE, # enable tensorboard 
#                            log = FALSE,
#                            serialize_model = FALSE, 
#                            full_model = FALSE),       
#              output_format = "target_right" 
# )

## open tensorboard in browser
# tensorflow::tensorboard(tensorboard_path)
```

The "SCALARS" tab displays accuracy, loss, learning rate and percentage of files seen for each epoch.

The "TEXT" tab shows the `trainNetwork` call as text.

The "HPARAM" tab tracks the hyperparameters of the different runs (maxlen, batch size etc.).

Further tensorboard documentation can be found [here](https://www.tensorflow.org/tensorboard/get_started).

## Checkpoints

We can save the architecture and weights of a model after every epoch using checkpoints. The checkpoints get stored in 
h5 format. The file names contain the corresponding epoch, loss and accuracy 

```{r warning = FALSE, message = FALSE}
# checkpoint_path <- file.path(dir_path, "checkpoints")
# if (!dir.exists(checkpoint_path)) dir.create(checkpoint_path)
# model <- create_model_lstm_cnn()
# run.name <- "run_2"
# trainNetwork(train_type = "lm", 
#              model = model,
#              path = train_path_1,
#              path.val = validation_path_1,
#              steps.per.epoch = 5, 
#              batch.size = 8,
#              epochs = 10,
#              run.name = run.name,
#              checkpoint_path = checkpoint_path,  
#              save_best_only = TRUE, # only save model if loss improves
#              save_weights_only = FALSE, # save architecture and weights
#              output = list(none = FALSE,
#                            checkpoints = TRUE, # enable checkpoints
#                            tensorboard = FALSE,  
#                            log = FALSE,
#                            serialize_model = FALSE, 
#                            full_model = FALSE),       
#              output_format = "target_right" 
# )
```

After training, we can now load a trained model and continue training or use the model for predictions/inference.

```{r warning = FALSE, message = FALSE}
# cp_run_path <- file.path(checkpoint_path, paste0(run.name, "_checkpoints"))
# checkpoints <- list.files(cp_run_path)
# checkpoints
# last_checkpoint <- checkpoints[length(checkpoints)]
# 
# # load trained model and compile
# model <- keras::load_model_hdf5(file.path(cp_run_path, last_checkpoint), compile = FALSE)
# model <- keras::load_weights_model_hdf5(model, file.path(cp_run_path, last_checkpoint))
# optimizer <-  keras::optimizer_adam(lr = 0.01)
# model %>% keras::compile(loss = "categorical_crossentropy", optimizer = optimizer, metrics = c("acc"))
# 
# # continue training
# trainNetwork(train_type = "lm", 
#              model = model,
#              path = train_path_1,
#              path.val = validation_path_1,
#              steps.per.epoch = 5, 
#              batch.size = 8,
#              epochs = 2,
#              run.name = "continue_from_checkpoint",
#              output_format = "target_right" 
# )
```

