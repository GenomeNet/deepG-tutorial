---
title: "deepG tutorial"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
  html_document:
    theme: united
    toc: yes
---

```{r setup, include=FALSE}
set.seed(321)
library(hdf5r)
library(magrittr)
library(ggplot2)
library(deepG)
```

# Introduction

The deepG library can be used for applying deep learning on genomic data. The library supports creating neural network architecture,
automation of data preprocesing (data generator), network training, inference and visualizing feature importances (integrated gradients). 

# Create a model

deepG supports three functions to create a keras model. 

## `create_model_lstm_cnn`

The architecture of this model is $k$ * LSTM, $m$ * CNN and $n$ * dense layers, where $k,m \ge 0$ and $n \ge 1$.   
The user can choose the size of the individual LSTM, CNN and Dense layers and add additional features to each layer;
for example the LSTM layer may be bidirectional (runs input in two ways) or stateful (considers dependencies between batches).  

The last dense layer layer has a softmax activation and determines how many targets we want to predict. This output gives a vector of probabilities, i.e. the sum 
of the vector is 1 and each entry is a probabilty for one class.  

The following implementation creates a model with 3 CNN layer (+ batch normalization), 1 LSTM and 1 dense layer.  
```{r}
model <- create_model_lstm_cnn(
  maxlen = 500, # number of nucleotides processed in one sample
  layer_lstm = c(32), # number of LSTM cells
  layer_dense = c(4), # number of neurons in last layer (4 targets: A,C,G,T)
  vocabulary.size = 4, # input vocabulary has size 4 (A,C,G,T)
  kernel_size = c(12, 12, 12), # size of individual CNN windows for each layer
  filters = c(32, 64, 64), # number of CNN filters per layer
  pool_size = c(3, 3, 3) # size of max pooling per layer
)
```

The model expects an input with dimensions (NULL (batch size), maxlen, vocabulary size) and a target with dimension (NULL (batch size), number of targets).
Maxlen specifies the length of the input sequence. 

```{r warning=FALSE}
batch_size <- 3
maxlen <- 500
vocabulary.size <- 4
input <- array(rnorm(maxlen * batch_size * vocabulary.size), 
               dim = c(batch_size, maxlen, vocabulary.size))
pred <- predict(model, input) # make a prediction with random data
dim(pred) 
colnames(pred) <- c("A", "C", "G", "T") 
pred # prediction for initial random weights
``` 

## `create_model_lstm_cnn_target_middle`

This architecture is closely related to `create_model_lstm_cnn_target` with the main difference that the model has two input layers (provided `label_input = NULL`).  

```{r}
model <- create_model_lstm_cnn_target_middle(
  maxlen = 500,
  layer_lstm = c(32),
  layer_dense = c(4),
  vocabulary.size = 4,
  kernel_size = c(12, 12, 12),
  filters = c(32, 64, 64),
  pool_size = c(3, 3, 3)
)
```

This architecture can be used to predict a character in the middle of a sequence. For example 

sequence: \texttt{ACCG}\texttt{\color{blue}T}\texttt{GGAA}

then the first input should correspond to \texttt{ACCG}, the second input to \texttt{GGAA} and \texttt{T} to the target.  
This can be used to combine the 2 tasks 

1. predict \texttt{T} given \texttt{ACCG} 

2. predict \texttt{T} given \texttt{AAGG} (note reversed order of input)

in one model.

## `create_model_wavenet`

This model uses causal dilated convolution layers, which is suitable to handle long sequences. The original paper can be found [here](https://arxiv.org/pdf/1609.03499.pdf)  

```{r warning = FALSE}
model <- create_model_wavenet(filters = 16, kernel_size = 2, residual_blocks = 2^(2:4),
                              maxlen = 500, input_tensor = NULL, initial_kernel_size = 32,
                              initial_filters = 32, output_channels = 4,
                              output_activation = "softmax", solver = "adam",
                              learning.rate = 0.001, compile = TRUE) 
model
```

The model expects an input and output of dimension (batch size, maxlen, vocabulary.size). The target sequence should be equal to input sequence shifted by 
one position. For example, given a sequence \texttt{ACCGGTC} and maxlen = 6, the input should correspond to \texttt{ACCGGT} and target to \texttt{CCGGTC}. 

# Training 

## Preparing the data

Input data must be files in FASTA or FASTQ format and file names must have .fasta or .fastq ending; otherwise files will be ignored. 
All training and validation data should each be in one folder. deepG uses a data generator to iterate over files in train/validation folder.   
Before we train our model, we have to decide what our training objetive is. It can be either a language model or label classification.

```{r warning = FALSE}
path <- "/home/rmreches/tutorial"
path_16S_train <- file.path(path, "16s/train")
path_16S_validation <- file.path(path, "16s/validation")
path_bacteria_train <- file.path(path, "bacteria/train")
path_bacteria_validation <- file.path(path, "bacteria/validation")

checkpoint_path <- file.path(path, "checkpoints")
tensorboard.log <- file.path(path, "tensorboard")
dir_path <- file.path(path, "outputs")
if (!dir.exists(checkpoint_path)) dir.create(checkpoint_path)
if (!dir.exists(tensorboard.log)) dir.create(tensorboard.log)
if (!dir.exists(dir_path)) dir.create(dir_path)
```

## Language model

With language model, we mean a model that predicts a character in a sequence.
The target can be at the end of the sequence, for example

\texttt{ACGTCA}\texttt{\color{blue}G}

or in the middle

\texttt{ACG}\texttt{\color{blue}T}\texttt{CAG}

### Language model for 16S (predict next character) 

Say we want to predict the next character in a sequence given the last 500 characters and our text consists of the letters \texttt{A,C,G,T}. First we have to create a model. We may use a model with 1 LSTM, 3 CNN and 1 dense layer for predictions.   

```{r warning = FALSE}
model <- create_model_lstm_cnn(
  maxlen = 500,
  layer_lstm = c(32),
  layer_dense = c(4),
  vocabulary.size = 4,
  kernel_size = c(12, 12, 12),
  filters = c(32, 64, 64),
  pool_size = c(3, 3, 3),
  learning.rate = 0.001
)
```

Next we have to specify the location of our training and validation data and the output format of the data generator    

```{r warning = FALSE, message = FALSE}

trainNetwork(train_type = "lm", # train a language model
             model = model,
             path = path_16S_train, # location of trainig data
             path.val = path_16S_validation, # location of validation data
             checkpoint_path = checkpoint_path,
             tensorboard.log = tensorboard.log,
             validation.split = 0.2, # use 20% of samples for validation compared to train size
             run.name = "lm_16S_target_right",
             batch.size = 256, 
             epochs = 4, 
             steps.per.epoch = 10, # 1 epoch = 10 batches
             step = 500, # take a sample every 500 steps
             output = list(none = FALSE,
                           checkpoints = TRUE,
                           tensorboard = TRUE,
                           log = FALSE,
                           serialize_model = FALSE,
                           full_model = FALSE
             ),
             tb_images = TRUE,
             output_format = "target_right" # predict target at end of sequence 
)

tensorflow::tensorboard(tensorboard.log)
```

### Predict character in middle of sequence 

If we want to predict a character in the middle of a sequence and use LSTM layers, we should split our input into two layers. One layer handles the sequence 
before and one the input after the target. If, for example 

sequence: \texttt{ACCG}\texttt{\color{blue}{T}}\texttt{GGAA} 

then first input corresponds to \texttt{ACCG} and second to \texttt{AAGG}. We may create a model with two input layers using the `create_model_cnn_lstm_target_middle`

```{r warning = FALSE}
model <- create_model_lstm_cnn_target_middle(
  maxlen = 500,
  layer_lstm = c(32),
  layer_dense = c(4),
  vocabulary.size = 4,
  kernel_size = c(12, 12, 12),
  filters = c(32, 64, 64),
  pool_size = c(3, 3, 3),
  learning.rate = 0.001
)
```

The `trainNetwork` call is identical to the previous model, except we have to change the output format of the generator by setting `output_format = "target_middle_lstm"`. This reverses the order of the sequence after the target.  

```{r warning = FALSE, message = FALSE}

trainNetwork(train_type = "lm", # train a language model
             model = model,
             path = path_16S_train, # location of trainig data
             path.val = path_16S_validation, # location of validation data
             checkpoint_path = checkpoint_path,
             tensorboard.log = tensorboard.log,
             validation.split = 0.2, # use 20% of samples for validation compared to train size
             run.name = "lm_16S_target_middle_lstm",
             batch.size = 256, 
             epochs = 4, 
             steps.per.epoch = 10, # 1 epoch = 10 batches
             step = 500, # take a sample every 500 steps
             output = list(none = FALSE,
                           checkpoints = TRUE,
                           tensorboard = TRUE,
                           log = FALSE,
                           serialize_model = FALSE,
                           full_model = FALSE
             ),
             tb_images = TRUE,
             output_format = "target_middle_lstm" # predict character in middle of sequence  
)
```

## Label classification

With label classification, we describe the task of mapping a label to a sequence. 
For example: given the sequence \texttt{ACGACCG}, does the sequence belong to a viral or bacterial genome?

deepG offers two options to map a label to a sequence 

1. the label gets read from the fasta header

2. files from every class are in seperate folders

### Label by folder

We put all data from one class into separate folders. In the following example, we want to classify if a sequence belongs to 16s or bacterial genome. We have to put all 16s/bacteria files into their own folder. In this case the `path` and `path.val` arguments should be vectors, where each entry is the path to one class.    
```{r warning = FALSE}
model <- create_model_lstm_cnn(
  maxlen = 500,
  layer_lstm = c(32),
  layer_dense = c(2), # predict two classes 
  vocabulary.size = 4,
  kernel_size = c(12, 12, 12),
  filters = c(32, 64, 64),
  pool_size = c(3, 3, 3),
  learning.rate = 0.001
)
```

```{r warning = FALSE, message = FALSE}
trainNetwork(train_type = "label_folder", # reading label from folder  
             model = model,
             path = c(path_16S_train, # note that path has two entries 
                      path_bacteria_train), 
             path.val = c(path_16S_validation,
                          path_bacteria_validation),
             checkpoint_path = checkpoint_path,
             tensorboard.log = tensorboard.log,
             validation.split = 0.2, 
             run.name = "16S_vs_bacteria",
             batch.size = 512, # half of batch is 16s and other half bacteria data
             epochs = 5, 
             steps.per.epoch = 15, 
             step = 500,
             labelVocabulary = c("16s", "bacteria"), # label names
             output = list(none = FALSE,
                           checkpoints = TRUE,
                           tensorboard = TRUE,
                           log = FALSE,
                           serialize_model = FALSE,
                           full_model = FALSE
             ),
             tb_images = TRUE,
             proportion_per_file = c(1, 0.05) # randomly select 5% of bacteria file 
)
```

# Inference

Once we have trained a model, we may use the model to get the activations of a certain layer and write the states to an h5 file.
In the following example we use the binary model trained to classify 16S/bacteria data.

```{r warning = FALSE, message = FALSE}
print(model)
num_layers <- length(model$get_config()$layers)
layer_name <- model$get_config()$layers[[num_layers]]$name
cat("get output at layer", layer_name)
fasta.path <- list.files(path_16S_validation, full.names = TRUE)[1] # make predictions for 16S file
fasta.file <- microseq::readFasta(fasta.path)
head(fasta.file)
sequence <- fasta.file$Sequence[1]
filename <- file.path(dir_path, "states.h5")

if (!file.exists(filename)) {
  writeStates(
    model = model,
    layer_name = layer_name, 
    sequence = sequence,
    round_digits = 4,
    filename = filename,
    batch.size = 10,
    mode = "lm")
}  
```

We can access the h5 file as follows

```{r warning = FALSE, message = FALSE}
states <- readRowsFromH5(h5_path = filename, complete = TRUE)
colnames(states) <- c("16S", "bacteria")
head(states)
```

The matrix shows the models confidence in its predictions. Every row corresponds to one sample. If the value in the 
16s column is > 0.500, the model will classify the sample as 16s.

## Detect 16S region 

We can use or trained model to detect 16S sequences in a bacterial genome. First, we search for the true rRNA region in the corresponding gff file.

```{r warning = FALSE, message = FALSE}
fasta.path<- file.path(path, "E_faecalis.fasta")
gff.file <- file.path(path, "E_faecalis.gff")
gff.data <- rtracklayer::readGFF(gff.file, version = 0,
                                 columns = NULL, tags = NULL, filter = NULL, nrows = -1,
                                 raw_data = FALSE)
rRNA_index <- stringr::str_detect(gff.data$product, "^16S ribosomal") & (gff.data$strand == "+")
start <- gff.data[rRNA_index, "start"]
end <- gff.data[rRNA_index, "end"]
start; end
```

We iterate over the bacteria file and make a predictions every 100 steps

```{r warning = FALSE, message = FALSE}
fasta.file <- microseq::readFasta(fasta.path)
sequence <- fasta.file$Sequence[1]
filename <- file.path(dir_path, "bacteria_states.h5")

if (!file.exists(filename)) {
  writeStates(
    model = model,
    layer_name = layer_name, 
    sequence = sequence,
    round_digits = 4,
    filename = filename,
    batch.size = 500,
    step = 100)
}  
```

```{r warning = FALSE, message = FALSE}
states <- readRowsFromH5(h5_path = filename, complete = TRUE, getTargetPositions = TRUE)
pred <- states[[1]]
position <- states[[2]] - 1
df <- cbind(pred, position) %>% as.data.frame()
colnames(df) <- c("conf_16S", "conf_bacteria", "seq_end")
head(df)
index_16S_pred <- df[ , 1] > 0.5
df_16S <- df[index_16S_pred, ]
head(df_16S)
```

Let's visualize or models predictions and compare them to the true areas. First we look at the confidence in 16S over the whole genome. 

```{r warning = FALSE, message = FALSE}
ggplot(df, aes(x = seq_end, y = conf_16S)) + geom_point() + geom_line() + ylab("16S confidence") + xlab("sequence position")
```

Next we may zoom into areas with high 16S confidence, the true 16S regions are shaded grey.

```{r warning = FALSE, message = FALSE}
p1 <- ggplot(df, aes(x = seq_end, y = conf_16S)) + geom_point() + geom_line() +
  geom_rect(aes(xmin=start[1], xmax=end[1], ymin=0, ymax=Inf),  alpha = 0.01) + 
  xlim(c(start[1] - 500, end[1] + 500)) + 
  ylab("16S confidence") + xlab("sequence position")
p2 <- ggplot(df, aes(x = seq_end, y = conf_16S)) + geom_point() + geom_line() +
  geom_rect(aes(xmin=start[2], xmax=end[2], ymin=0, ymax=Inf),  alpha = 0.01) + 
  xlim(c(start[2] - 500, end[2] + 500)) + 
  ylab("16S confidence") + xlab("sequence position")
ggpubr::ggarrange(p1, p2, ncol = 1, nrow = 2)
```

# Tensorboard

We can use tensorboard to monitor our training runs. To track the runs, we have to specify a path for tensorboard files and give the run a unique name.   

```{r warning = FALSE, message = FALSE}
# trainNetwork(run.name = "unique_run_name",
#              tensorboard.log = "tensorboard_path",
#              ...
# )
```

We can inspect out previous training runs in tensorboard 

```{r warning = FALSE, message = FALSE}
## open tensorboard in browser
# tensorflow::tensorboard(tensorboard.log)
```

The "SCALARS" tab displays accuracy,  

![accuracy](~/tutorial/images/accuracy.png)

loss

![loss](~/tutorial/images/loss.png)

and percentage of files seen for each epoch

![percentage of seen training files](~/tutorial/images/file_perc.png)

In the "IMAGES" tab, we implemented a display of train and validation confusion matrices after every epoch.
We can see for our binary classification of bacteria/16S sequences, that the model misclassifies more bacteria sequences as 
16S than vice versa.

![confusion matrix](~/tutorial/images/cm.png)

The "TEXT" tab shows the `trainNetwork` call as text.

![trainNetwork call](~/tutorial/images/text.png)

The "HPARAM" tab tracks the hyperparameters of the different runs (maxlen, batch size etc.). This can be used to find 
the hyperparameter settings for a given task

![hyperparameters](~/tutorial/images/hp.png)

Further tensorboard documentation can be found [here](https://www.tensorflow.org/tensorboard/get_started).

# Checkpoints

We can save the architecture and weights of a model after every epoch using checkpoints. The checkpoints get stored in 
h5 format. The file names contain the corresponding epoch, loss and accuracy. For example, we can display the checkpoints from
binary classification model for 16S/bacteria.

```{r warning = FALSE, message = FALSE}
cp <- list.files(file.path(checkpoint_path, "16S_vs_bacteria_checkpoints"), full.names = TRUE)
print(basename(cp))
```

After training, we can load a trained model and continue training or use the model for predictions/inference.
Let's create a model with random weights identical to our 16S/bacteria classifier and make some predictions.

```{r warning = FALSE, message = FALSE}
model <- create_model_lstm_cnn(
  maxlen = 500,
  layer_lstm = c(32),
  layer_dense = c(2), 
  vocabulary.size = 4,
  kernel_size = c(12, 12, 12),
  filters = c(32, 64, 64),
  pool_size = c(3, 3, 3),
  learning.rate = 0.001
)

eval_model <- evaluateFasta(fasta.path = c(path_16S_validation,
                             path_bacteria_validation),
              model = model,
              batch.size = 100,
              step = 100,
              label_vocabulary = c("16s", "bacteria"),
              numberOfBatches = 10,
              mode = "label_folder")
eval_model[["accuracy"]]
eval_model[["confusion_matrix"]]
```

As expected, the performance is not better than random guessing. Let's repeat evaluation but load the weights
of our pretrained model

```{r warning = FALSE, message = FALSE}
weight_path <- cp[length(cp)]
model <- keras::load_model_weights_hdf5(model, weight_path)

eval_model <- evaluateFasta(fasta.path = c(path_16S_validation,
                             path_bacteria_validation),
              model = model,
              batch.size = 100,
              step = 100,
              label_vocabulary = c("16s", "bacteria"),
              numberOfBatches = 10,
              mode = "label_folder")
eval_model[["accuracy"]]
eval_model[["confusion_matrix"]]
```

# Integrated gradient