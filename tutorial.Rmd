---
title: "deepG tutorial"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
  html_document:
    theme: united
    toc: yes
---

```{r setup, include=FALSE}
set.seed(321)
library(hdf5r)
library(magrittr)
library(ggplot2)
library(deepG)
library(reticulate)
```

# Introduction

The deepG library can be used for applying deep learning on genomic data. The library supports creating neural network architecture,
automation of data preprocessing (data generator), network training, inference and visualizing feature importance (integrated gradients). 

# Create a model

deepG supports three functions to create a keras model. 

## `create_model_lstm_cnn`

The architecture of this model is $k$ * LSTM, $m$ * CNN and $n$ * dense layers, where $k,m \ge 0$ and $n \ge 1$.  
LSTM (long short-term memory) layers are specifically designed to processes sequential data (order of data is important) by using feedback connections.
CNN (convolutional neural network) layers are usually applied to images or audio data but can also be used for natural language processing 
or genomic sequences. Contrary to vanilla feedforward networks, they are able to process spatial relations in the data.  
The user can choose the size of the individual LSTM, CNN and Dense layers and add additional features to each layer;
for example the LSTM layer may be bidirectional (runs input in two ways) or stateful (considers dependencies between batches).  

The last dense layer has a softmax activation and determines how many targets we want to predict. This output gives a vector of probabilities, i.e. the sum 
of the vector is 1 and each entry is a probability for one class.  

The following implementation creates a model with 3 CNN layer (+ batch normalization and max pooling), 1 LSTM and 1 dense layer.  
```{r}
model <- create_model_lstm_cnn(
  maxlen = 500, # number of nucleotides processed in one sample
  layer_lstm = c(32), # number of LSTM cells
  layer_dense = c(4), # number of neurons in last layer (4 targets: A,C,G,T)
  vocabulary.size = 4, # input vocabulary has size 4 (A,C,G,T)
  kernel_size = c(12, 12, 12), # size of individual CNN windows for each layer
  filters = c(32, 64, 64), # number of CNN filters per layer
  pool_size = c(3, 3, 3) # size of max pooling per layer
)
```

![model with 3 CNN and 1 LSTM and 1 Dense layer](~/tutorial/images/model.png)

The model expects an input of dimensions (NULL (batch size), maxlen, vocabulary size) and a target of dimension (NULL (batch size), number of targets).
Maxlen specifies the length of the input sequence. 

```{r warning=FALSE}
batch_size <- 3
maxlen <- 500
vocabulary.size <- 4
input <- array(rnorm(maxlen * batch_size * vocabulary.size), 
               dim = c(batch_size, maxlen, vocabulary.size))
pred <- predict(model, input) # make a prediction with random data
dim(pred) 
colnames(pred) <- c("A", "C", "G", "T") 
pred # prediction for initial random weights
``` 

## `create_model_lstm_cnn_target_middle`

This architecture is closely related to `create_model_lstm_cnn_target` with the main difference that the model has two input layers (provided `label_input = NULL`).  

```{r}
model <- create_model_lstm_cnn_target_middle(
  maxlen = 500,
  layer_lstm = c(32),
  layer_dense = c(4),
  vocabulary.size = 4,
  kernel_size = c(12, 12, 12),
  filters = c(32, 64, 64),
  pool_size = c(3, 3, 3)
)
```

![model with two input layers](~/tutorial/images/model_target_middle.png)

This architecture can be used to predict a character in the middle of a sequence. For example 

sequence: \texttt{ACCG}\texttt{\color{blue}T}\texttt{GGAA}

then the first input should correspond to \texttt{ACCG}, the second input to \texttt{GGAA} and \texttt{T} to the target.  
This can be used to combine the 2 tasks 

1. predict \texttt{T} given \texttt{ACCG} 

2. predict \texttt{T} given \texttt{AAGG} (note reversed order of input)

in one model.

## `create_model_wavenet`

This model uses causal dilated convolution layers, which is suitable to handle long sequences. The original paper can be found [here](https://arxiv.org/pdf/1609.03499.pdf)  

```{r warning = FALSE}
model <- create_model_wavenet(filters = 16, kernel_size = 2, residual_blocks = 2^(2:4),
                              maxlen = 500, input_tensor = NULL, initial_kernel_size = 32,
                              initial_filters = 32, output_channels = 4,
                              output_activation = "softmax", solver = "adam",
                              learning.rate = 0.001, compile = TRUE) 
model
```

The model expects an input and output of dimension (batch size, maxlen, vocabulary.size). The target sequence should be equal to input sequence shifted by 
one position. For example, given a sequence \texttt{ACCGGTC} and maxlen = 6, the input should correspond to \texttt{ACCGGT} and target to \texttt{CCGGTC}. 

# Training 

## Preparing the data

Input data must be files in FASTA or FASTQ format and file names must have .fasta or .fastq ending; otherwise files will be ignored. 
All training and validation data should each be in one folder. deepG uses a data generator to iterate over files in train/validation folder.   
Before we train our model, we have to decide what our training objective is. It can be either a language model or label classification.

```{r warning = FALSE}
path <- "/home/rmreches/tutorial"
path_16S_train <- file.path(path, "16s/train")
path_16S_validation <- file.path(path, "16s/validation")
path_bacteria_train <- file.path(path, "bacteria/train")
path_bacteria_validation <- file.path(path, "bacteria/validation")

checkpoint_path <- file.path(path, "checkpoints")
tensorboard.log <- file.path(path, "tensorboard")
dir_path <- file.path(path, "outputs")
if (!dir.exists(checkpoint_path)) dir.create(checkpoint_path)
if (!dir.exists(tensorboard.log)) dir.create(tensorboard.log)
if (!dir.exists(dir_path)) dir.create(dir_path)
```

Our data set for this tutorial consists of 16S sequences and bacterial genomes. 

```{r warning = FALSE}
cat("number of files in 16S train:",  length(list.files(path_16S_train)), "\n")
cat("number of files in 16S validation:", length(list.files(path_16S_validation)), "\n")
cat("number of files in bacteria train:", length(list.files(path_bacteria_train)), "\n")
cat("number of files in bacteria validation:",  length(list.files(path_bacteria_validation)), "\n")

# print first 16S file 
print(microseq::readFasta(list.files(path_16S_train, full.names = TRUE)[1]))

seq_length_16_train <- vector("integer", length(path_16S_train))
i <- 1
for (file_name in list.files(path_16S_train, full.names = TRUE)) {
  fasta.file <- microseq::readFasta(file_name)
  seq_length_16_train[i] <- nchar(fasta.file$Sequence)
  i <- i + 1
}
df_16S <- as.data.frame(seq_length_16_train)

seq_length_bacteria_train <- vector("integer", length(path_bacteria_train))
i <- 1
for (file_name in list.files(path_bacteria_train, full.names = TRUE)) {
  fasta.file <- microseq::readFasta(file_name)
  seq_length_bacteria_train[i] <- nchar(fasta.file$Sequence)
  i <- i + 1
}
df_bact <- as.data.frame(seq_length_bacteria_train)

p1 <- ggplot(df_16S, aes(x=seq_length_16_train)) +
  xlab("length of 16S sequences") + 
  geom_histogram(bins=30)
p2 <- ggplot(df_bact, aes(x=seq_length_bacteria_train)) + 
  xlab("length of bacteria sequences") + 
  geom_histogram(bins=20)
ggpubr::ggarrange(p1, p2, ncol = 2, nrow = 1)
```


## Language model

With language model, we mean a model that predicts a character in a sequence.
The target can be at the end of the sequence, for example

\texttt{ACGTCA}\texttt{\color{blue}G}

or in the middle

\texttt{ACG}\texttt{\color{blue}T}\texttt{CAG}

### Language model for 16S (predict next character) 

Say we want to predict the next character in a sequence given the last 500 characters and our text consists of the letters \texttt{A,C,G,T}. First we have to create a model. We may use a model with 1 LSTM, 3 CNN and 1 dense layer for predictions.   

```{r warning = FALSE, message = FALSE, results = 'hide'}
model <- create_model_lstm_cnn(
  maxlen = 500,
  layer_lstm = c(32),
  layer_dense = c(4),
  vocabulary.size = 4,
  kernel_size = c(12, 12, 12),
  filters = c(32, 64, 128),
  pool_size = c(3, 3, 3),
  learning.rate = 0.001
)
```

Next we have to specify the location of our training and validation data and the output format of the data generator. We 
randomly select 95% from each file so the generator does not take samples from the same position repeatedly.

```{r warning = FALSE, message = FALSE}

trainNetwork(train_type = "lm", # train a language model
             model = model,
             path = path_16S_train, # location of trainig data
             path.val = path_16S_validation, # location of validation data
             checkpoint_path = checkpoint_path,
             tensorboard.log = tensorboard.log,
             validation.split = 0.2, # use 20% of  train size for validation
             run.name = "lm_16S_target_right",
             batch.size = 256, 
             epochs = 5, 
             steps.per.epoch = 15, # 1 epoch = 15 batches
             step = 100, # take a sample every 100 steps
             output = list(none = FALSE,
                           checkpoints = TRUE,
                           tensorboard = TRUE,
                           log = FALSE,
                           serialize_model = FALSE,
                           full_model = FALSE
             ),
             tb_images = TRUE,
             output_format = "target_right", # predict target at end of sequence 
             proportion_per_file = c(0.95)
             # randomly select 95% of file 
)

# tensorflow::tensorboard(tensorboard.log)
```

### Predict character in middle of sequence 

If we want to predict a character in the middle of a sequence and use LSTM layers, we should split our input into two layers. One layer handles the sequence 
before and one the input after the target. If, for example 

sequence: \texttt{ACCG}\texttt{\color{blue}{T}}\texttt{GGAA} 

then first input corresponds to \texttt{ACCG} and second to \texttt{AAGG}. We may create a model with two input layers using the `create_model_cnn_lstm_target_middle`

```{r warning = FALSE, message = FALSE, results = 'hide'}
model <- create_model_lstm_cnn_target_middle(
  maxlen = 500,
  layer_lstm = c(32),
  layer_dense = c(4),
  vocabulary.size = 4,
  kernel_size = c(12, 12, 12),
  filters = c(32, 64, 128),
  pool_size = c(3, 3, 3),
  learning.rate = 0.001
)
```

The `trainNetwork` call is identical to the previous model, except we have to change the output format of the generator by setting `output_format = "target_middle_lstm"`. This reverses the order of the sequence after the target.  

```{r warning = FALSE, message = FALSE}

trainNetwork(train_type = "lm", # train a language model
             model = model,
             path = path_16S_train, # location of trainig data
             path.val = path_16S_validation, # location of validation data
             checkpoint_path = checkpoint_path,
             tensorboard.log = tensorboard.log,
             validation.split = 0.2, # use 20% of train size for validation
             run.name = "lm_16S_target_middle_lstm",
             batch.size = 256, 
             epochs = 5, 
             steps.per.epoch = 15, # 1 epoch = 15 batches
             step = 100, # take a sample every 100 steps
             output = list(none = FALSE,
                           checkpoints = TRUE,
                           tensorboard = TRUE,
                           log = FALSE,
                           serialize_model = FALSE,
                           full_model = FALSE
             ),
             tb_images = TRUE,
             output_format = "target_middle_lstm", # predict character in middle 
             proportion_per_file = c(0.95)
             # randomly select 95% of file 
)
```

## Label classification

With label classification, we describe the task of mapping a label to a sequence. 
For example: given the sequence \texttt{ACGACCG}, does the sequence belong to a viral or bacterial genome?

deepG offers two options to map a label to a sequence 

1. the label gets read from the fasta header

2. files from every class are in separate folders

![Example: read class label from header](~/tutorial/images/header.png)

### Label by folder

We put all data from each class into separate folders. In the following example, we want to classify if a sequence belongs to 16s or bacterial genome. We have to put all 16S/bacteria files into their own folder. In this case the `path` and `path.val` arguments should be vectors, where each entry is the path to one class.    
```{r warning = FALSE, results = 'hide'}
model <- create_model_lstm_cnn(
  maxlen = 500,
  layer_lstm = NULL,
  layer_dense = c(2), # predict 2 classes
  vocabulary.size = 4,
  kernel_size = c(12, 12, 12),
  filters = c(32, 64, 128),
  pool_size = c(3, 3, 3),
  learning.rate = 0.001
)
```

```{r warning = FALSE, message = FALSE}
trainNetwork(train_type = "label_folder", # reading label from folder  
             model = model,
             path = c(path_16S_train, # note that path has two entries 
                      path_bacteria_train), 
             path.val = c(path_16S_validation,
                          path_bacteria_validation),
             checkpoint_path = checkpoint_path,
             tensorboard.log = tensorboard.log,
             validation.split = 0.2, 
             run.name = "16S_vs_bacteria",
             batch.size = 256, # half of batch is 16S and other half bacteria data
             epochs = 5, 
             save_best_only = FALSE,
             steps.per.epoch = 15, 
             step = c(100, 500), # smaller step size for 16S
             labelVocabulary = c("16s", "bacteria"), # label names
             output = list(none = FALSE,
                           checkpoints = TRUE,
                           tensorboard = TRUE,
                           log = FALSE,
                           serialize_model = FALSE,
                           full_model = FALSE
             ),
             tb_images = TRUE,
             proportion_per_file = c(0.95, 0.05)
             # randomly select 95% of 16S and 5% of bacteria files, 
             # since bacteria files are much larger  
)
```

# Checkpoints

We can save the architecture and weights of a model after every epoch using checkpoints. The checkpoints get stored in 
h5 format. The file names contain the corresponding epoch, loss and accuracy. For example, we can display the checkpoints from
binary classification model for 16S/bacteria.

```{r warning = FALSE, message = FALSE}
cp <- list.files(file.path(checkpoint_path, "16S_vs_bacteria_checkpoints"),
                 full.names = TRUE)
print(basename(cp))
```

After training, we can load a trained model and continue training or use the model for predictions/inference.
Let's create a model with random weights identical to our 16S/bacteria classifier and make some predictions.

```{r warning = FALSE, message = FALSE, results = 'hide'}
model <- create_model_lstm_cnn(
  maxlen = 500,
  layer_lstm = NULL,
  layer_dense = c(2), 
  vocabulary.size = 4,
  kernel_size = c(12, 12, 12),
  filters = c(32, 64, 128),
  pool_size = c(3, 3, 3),
  learning.rate = 0.001
)

# evaluate 1000 samples, 500 from each class 
eval_model <- evaluateFasta(fasta.path = c(path_16S_validation,
                                           path_bacteria_validation),
                            model = model,
                            batch.size = 100,
                            step = 100,
                            label_vocabulary = c("16s", "bacteria"),
                            numberOfBatches = 10,
                            mode = "label_folder",
                            verbose = FALSE)
```

```{r warning = FALSE, message = FALSE}
eval_model[["accuracy"]]
eval_model[["confusion_matrix"]]
```

As expected, the performance is not better than random guessing. Let's repeat evaluation but load the weights
of our pretrained model

```{r warning = FALSE, message = FALSE, results = 'hide'}
weight_path <- cp[length(cp)]
model <- keras::load_model_weights_hdf5(model, weight_path)
eval_model <- evaluateFasta(fasta.path = c(path_16S_validation,
                                           path_bacteria_validation),
                            model = model,
                            batch.size = 100,
                            step = 100,
                            label_vocabulary = c("16s", "bacteria"),
                            numberOfBatches = 10,
                            mode = "label_folder",
                            verbose = FALSE)
```


```{r warning = FALSE, message = FALSE}
eval_model[["accuracy"]]
eval_model[["confusion_matrix"]]
```

# Inference

Once we have trained a model, we may use the model to get the activations of a certain layer and write the states to an h5 file.
First, we apply our model to a file from our 16S validation set.

```{r warning = FALSE, message = FALSE, results = 'hide'}
model <- keras::load_model_hdf5(weight_path, compile = FALSE)
model <- keras::load_model_weights_hdf5(model, weight_path)
```


```{r warning = FALSE, message = FALSE}
maxlen <- model$input$shape[[2]]
num_layers <- length(model$get_config()$layers)
layer_name <- model$get_config()$layers[[num_layers]]$name
cat("get output at layer", layer_name)
fasta.path <- list.files(path_16S_validation, # make predictions for 16S file
                         full.names = TRUE)[1] 
fasta.file <- microseq::readFasta(fasta.path)
head(fasta.file)
sequence <- fasta.file$Sequence[1]
filename <- file.path(dir_path, "states.h5")
```

```{r warning = FALSE, message = FALSE, results = 'hide'}
if (!file.exists(filename)) {
  writeStates(
    model = model,
    layer_name = layer_name, 
    sequence = sequence,
    round_digits = 4,
    filename = filename,
    batch.size = 10,
    mode = "lm")
}  
```

We can access the h5 file as follows

```{r warning = FALSE, message = FALSE}
states <- readRowsFromH5(h5_path = filename, complete = TRUE)
colnames(states) <- c("conf_16S", "conf_bacteria")
rownames(states) <- paste0("sample_", 1:nrow(states))
head(states)
ggplot(as.data.frame(states)) +
  geom_histogram(aes(conf_16S, color = "16S", fill = "16S"), alpha = 0.4) +
  geom_histogram(aes(conf_bacteria, color = "bacteria", fill = "bacteria"), alpha = 0.4) +
  xlab("confidence") + ggtitle("model confidence for 16S file")
```

The matrix shows the models confidence in its predictions. Every row corresponds to one sample. If the value in the 
16s column is > 0.500, the model will classify the sample as 16s.

## Detect 16S region 

In the following example we use the binary model trained to classify 16S/bacteria to predict the location of 16S sequences (similar to https://github.com/tseemann/barrnap). We use the same model architecture as before, but load the weights of a model that was trained on more data and for a longer time. We iterate over a new bacteria file (not present in train or validation data) and make predictions every 100 steps

```{r warning = FALSE, message = FALSE, include = FALSE}
# load pretrained model
model_path <- "pretrained_models/bact_16S_model.hdf5"
model <- keras::load_model_hdf5(model_path, compile = FALSE)
model <- keras::load_model_weights_hdf5(model, model_path)

fasta.path <- file.path(path, "E_faecalis.fasta")
fasta.file <- microseq::readFasta(fasta.path)
sequence <- fasta.file$Sequence[1]
filename <- file.path(dir_path, "bacteria_states.h5")
```

```{r warning = FALSE, message = FALSE, results = 'hide'}
if (!file.exists(filename)) {
  writeStates(
    model = model,
    layer_name = layer_name, 
    sequence = sequence,
    round_digits = 4,
    filename = filename,
    batch.size = 500,
    step = 100)
}  
```


```{r warning = FALSE, message = FALSE}
states <- readRowsFromH5(h5_path = filename, complete = TRUE, getTargetPositions = TRUE)
pred <- states[[1]]
position <- states[[2]] - 1
df <- cbind(pred, position) %>% as.data.frame()
colnames(df) <- c("conf_16S", "conf_bacteria", "seq_end")
head(df)
index_16S_pred <- df[ , 1] > 0.5
df_16S <- df[index_16S_pred, ]
head(df_16S)
```

Next, we search for the true rRNA region in the corresponding gff file to validate our models predictions.

```{r warning = FALSE, message = FALSE}
fasta.path <- file.path(path, "E_faecalis.fasta")
gff.file <- file.path(path, "E_faecalis.gff")
gff.data <- rtracklayer::readGFF(gff.file, version = 0,
                                 columns = NULL, tags = NULL, filter = NULL, nrows = -1,
                                 raw_data = FALSE)
rRNA_index <- stringr::str_detect(gff.data$product, "^16S ribosomal") &
  (gff.data$strand == "+")
start <- gff.data[rRNA_index, "start"]
end <- gff.data[rRNA_index, "end"]
cat("start positions of 16S sequences:", start, "\n")
cat("end positions of 16S sequences:", end, "\n")
```

We can now plot the models confidence in 16S over the whole genome. 

```{r warning = FALSE, message = FALSE}
ggplot(df, aes(x = seq_end, y = conf_16S)) + geom_point() +
  geom_line() + ylab("16S confidence") + xlab("end position of sample") +
  geom_hline(yintercept=0.8, linetype="dashed", color = "red") +
  annotate("segment", xend = start[1], x = start[1], y = -0.2, yend = -0.02,
           colour = "blue", size = 1, arrow = arrow()) +
  annotate("segment", xend = start[2], x = start[2], y = -0.2, yend = -0.02,
           colour = "blue", size = 1, arrow = arrow()) +
  annotate("text", x = mean(c(start[1], start[2])), y = -0.1, label = "16S regions", color = "blue")
```

Next we may zoom into areas with high 16S confidence, the true 16S regions are shaded grey.

```{r warning = FALSE, message = FALSE}
p1 <- ggplot(df, aes(x = seq_end - 250, y = conf_16S)) + geom_point() + geom_line() +
  geom_rect(aes(xmin=start[1], xmax=end[1], ymin=0, ymax=Inf),  alpha = 0.01) + 
  xlim(c(start[1] - 500, end[1] + 500)) + 
  ylab("16S confidence") + xlab("sequence position")
p2 <- ggplot(df, aes(x = seq_end - 250, y = conf_16S)) + geom_point() + geom_line() +
  geom_rect(aes(xmin=start[2], xmax=end[2], ymin=0, ymax=Inf),  alpha = 0.01) + 
  xlim(c(start[2] - 500, end[2] + 500)) + 
  ylab("16S confidence") + xlab("sequence position")
ggpubr::ggarrange(p1, p2, ncol = 1, nrow = 2)
```

Finally, let's decrease our step size parameter to 1 and make a predictions for every possible sample around the true 16S regions 
(add 500 nucleotides to start/end)

```{r warning = FALSE, message = FALSE}
fasta.file <- microseq::readFasta(fasta.path)
buffer <- 500
sequence_16S_1 <- fasta.file$Sequence[1] %>% 
  substr(start = start[1] - buffer, stop = end[1] + buffer)
sequence_16S_2 <- fasta.file$Sequence[1] %>% 
  substr(start = start[2] - buffer, stop = end[2] + buffer)

filename_16S_1 <- file.path(dir_path, "states_16S_1.h5")
filename_16S_2 <- file.path(dir_path, "states_16S_2.h5")

if (!file.exists(filename_16S_1)) {
  writeStates(
    model = model,
    layer_name = layer_name, 
    sequence = sequence_16S_1,
    round_digits = 4,
    filename = filename_16S_1,
    batch.size = 500,
    step = 1)
}  

if (!file.exists(filename_16S_2)) {
  writeStates(
    model = model,
    layer_name = layer_name, 
    sequence = sequence_16S_2,
    round_digits = 4,
    filename = filename_16S_2,
    batch.size = 500,
    step = 1)
}  

states_1 <- readRowsFromH5(h5_path = filename_16S_1, complete = TRUE, getTargetPositions = TRUE)
pred_1 <- states_1[[1]]
position_1 <- start[1] - buffer + 1 + states_1[[2]] - floor(maxlen/2)
df_1 <- cbind(pred_1, position_1) %>% as.data.frame()
colnames(df_1) <- c("conf_16S", "conf_bacteria", "seq_end")
states_2 <- readRowsFromH5(h5_path = filename_16S_2, complete = TRUE, getTargetPositions = TRUE)
pred_2 <- states_2[[1]]
position_2 <-  start[2] - buffer + 1 + states_2[[2]] - floor(maxlen/2) 
df_2 <- cbind(pred_2, position_2) %>% as.data.frame()
colnames(df_2) <- c("conf_16S", "conf_bacteria", "seq_end")
```

and plot the results again 

```{r warning = FALSE, message = FALSE}
p1 <- ggplot(df_1, aes(x = seq_end, y = conf_16S)) + geom_point() + 
  geom_rect(aes(xmin=start[1], xmax=end[1], ymin=0, ymax=Inf),  alpha = 0.01) + 
  xlim(c(start[1] - buffer, end[1] + buffer)) + 
  ylab("16S confidence") + xlab("sequence position") + 
  geom_smooth()
p2 <- ggplot(df_2, aes(x = seq_end, y = conf_16S)) + geom_point() + 
  geom_rect(aes(xmin=start[2], xmax=end[2], ymin=0, ymax=Inf),  alpha = 0.01) + 
  xlim(c(start[2] - buffer, end[2] + buffer)) + 
  ylab("16S confidence") + xlab("sequence position") + 
  geom_smooth()
ggpubr::ggarrange(p1, p2, ncol = 1, nrow = 2)
```

# Tensorboard

We can use tensorboard to monitor our training runs. To track the runs, we have to specify a path for tensorboard files and give the run a unique name.   
```{r warning = FALSE, message = FALSE}
# trainNetwork(run.name = "unique_run_name",
#              tensorboard.log = "tensorboard_path",
#              ...
# )
```

We can inspect our previous training runs in tensorboard 

```{r warning = FALSE, message = FALSE}
## open tensorboard in browser
# tensorflow::tensorboard(tensorboard.log)
```

The "SCALARS" tab displays accuracy, loss and percentage of files seen for each epoch

![accuracy](~/tutorial/images/accuracy.png)

![loss](~/tutorial/images/loss.png)

![percentage of seen training files](~/tutorial/images/file_perc.png)

In the "IMAGES" tab, we implemented a display of train and validation confusion matrices after every epoch.
We can see for our binary classification of bacteria/16S sequences, that the model misclassified more bacteria sequences as 
16S than vice versa in one of the epochs.

![confusion matrix](~/tutorial/images/cm.png)

The "TEXT" tab shows the `trainNetwork` call as text.

![trainNetwork call](~/tutorial/images/text.png)

The "HPARAM" tab tracks the hyper parameters of the different runs (maxlen, batch size etc.). This can helpful to find 
the optimal hyper parameter settings for a given task

![hyperparameters](~/tutorial/images/hp.png)

Further tensorboard documentation can be found [here](https://www.tensorflow.org/tensorboard/get_started).

# Integrated gradient

To visualize which parts of an input sequence is important for the models decision, we may use a method called Integrated Gradient ([paper](https://arxiv.org/pdf/1703.01365.pdf)). This 
compares the predictions for a given sequence to a baseline sequence (usually a zero tensor or shuffling of original sequence) to determine 
which features were important for the models decision.

```{r warning = FALSE, message = FALSE, results = 'hide'}
# load trained model
model_path <- "pretrained_models/bact_16S_model.hdf5"
model <- keras::load_model_hdf5(model_path, compile = FALSE)
model <- keras::load_model_weights_hdf5(model, model_path)
```

```{r warning = FALSE, message = FALSE}
fasta_path <- file.path(path_16S_validation, 
                        "GCF_000427035.1_09mas018883_genomic.16s.fasta.fasta")
fasta_file <- microseq::readFasta(fasta_path)

# extract input tensors with data generator
gen <- labelByFolderGenerator(corpus.dir = fasta_path,
                              batch.size = 1,
                              maxlen = 500,
                              reverseComplements = FALSE,
                              numTargets = 2,
                              onesColumn = 1,
                              step = 1,
                              padding = FALSE)

ig_list <- list()
seq_len <- nchar(fasta_file$Sequence)
for (i in 1:(seq_len - 500)) {
  z <- gen()
  input <- z[[1]]
  ig <- integrated_gradients(m_steps = 50,
                             baseline_type = "shuffle",
                             input_seq = input,
                             target_class_idx = 1,
                             model = model,
                             num_baseline_repeats = 2)

  py$integrated_grads <- ig
  py_run_string("attribution_mask = tf.reduce_sum(tf.math.abs(integrated_grads), axis=-1)")
  attribution_mask <- py$attribution_mask
  attribution_mask <- as.matrix(attribution_mask, ncol = 1)
  df <- data.frame(position = (1:(nrow(attribution_mask))) + i - 1,
                   ig_sum = attribution_mask[ , 1])
  ig_list[[i]] <- df
}
#saveRDS(ig_list, dir_path)

ig_df <- data.table::rbindlist(ig_list) %>% as.data.frame()
ig_df <- aggregate(x = ig_df$ig_sum,
                          by = list(ig_df$position),
                          FUN = mean)
names(ig_df) <- c("position", "ig_sum")
ggplot(ig_df, aes(x = position, y = ig_sum, alpha = 0.001)) +
  #geom_point() +
  geom_smooth() + ylab("feature importance") +
  theme(legend.position = "none") + 
  geom_line()
```

